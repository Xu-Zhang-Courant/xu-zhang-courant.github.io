<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Xu's Website</title>
    <style>
        /* Styling the navigation bar */
        nav {
            background-color: #333;
            overflow: hidden;
        }
        nav a {
            float: left;
            display: block;
            color: white;
            text-align: center;
            padding: 14px 16px;
            text-decoration: none;
        }
        nav a:hover {
            background-color: #ddd;
            color: black;
        }

        /* Styling the headers */
        header {
            margin: 20px 0;
        }
        .flex-container {
            display: flex;
            align-items: center; /* Vertically aligns the image with the text */
        }
        .flex-container img {
            margin-right: 20px; /* Adjust spacing between the image and the text */
            max-width: 220px; /* Set a maximum width for the image */
            height: auto; /* Maintain the aspect ratio */
        }
        .flex-container p {
            margin: 0; /* Remove default margin */
        }
    </style>
</head>
    <header>
    <h1>Hi, I am Xu Zhang. Welcome to My Website! </h1>
    </header>
    
    <h2>Home</h2>
    
    <body>
        <div class="flex-container">
            <img src="https://github.com/Xu-Zhang-Courant/xu-zhang-courant.github.io/blob/main/images/self_casual.jpg?raw=true" alt="My Selfie" width="216" height="400">
            <br>
            <p>I am a CS Master student at Courant Institute of New York University (expected to graduate in 2026). Before this, I obtained my bachelor degree in mathematics at the University of Texas at Austin in 2024. I don't want my math skill be just a cynical, arcane art. Instead, I want my mathematical insight to have real-world impacts. " </p>
        </div>
        <br>
        <p> Here you can find my CV:</p>
        <a href="Resume_Xu_Zhang_Sep_18.pdf" target="_blank">Download My CV</a> <!-- "https://github.com/Xu-Zhang-Courant/xu-zhang-courant.github.io/blob/main/Resume_Xu_Zhang_Sep_18.pdf" -->

        <p>As a result, I eventually developed my interest in Few-Shot Learning (FSL), Continual Learning, and Transfer Learning. But Why FSL? </p>
        <br>
        <p>Fundamentally, we consistently face the challenges of the bias-complexity tradeoff. Having a large hypothesis space is a desirable and common practice since it can decrease the approximation error/inductive bias (the difference between the best solution in the hypothesis space and the universally best solution). However, a large hypothesis space also increases the risk of overfitting. Ever since Alexnet, people prefer to choose a large hypothesis space and use various techniques such regularization to prevent overfitting during the big data time. One crucial reason is that increasing the training sample size can decrease the estimation error (the error between the learned solution and the best solution possible within the hypothesis space), while the approximation error is solely dependent on the hypothesis class we choose. Thus, the advantage of massive data available to us in the big data time enabled us to choose a larger hypothesis space. <p>
        <p>However, there are two issues with this approach. <p>
        <p>First, massive data is not always available for tasks. For example, to detect rare flaws happening in Lithium batteries production, it’s common to have only a few dozens of available pictures for each flaw type, if not less. Without the large data size, we can not decrease the estimation error like before, and having a large hypothesis space exposes us to the threat of overfitting. <p>
        
        <p>Second, the training process with big data does not embody the essence of “learning” very well. Learning can only happen when limited information is presented, and humans do not need massive information to learn a task. <p>
        <p>To address these problems, there is the few-shot learning (FSL). Take embedding methods in FSL as an example. Through embedding functions, we can project limited available data into a smaller space that discriminates the data from each other more easily. In the smaller space, we can construct smaller hypotheses space and thus decrease the errors. A good smaller hypothesis space should contain good solutions equivalent to the one we had in the original hypothesis space. This goal depends on a good embedding function. Thus, we incorporate prior knowledge to learn the embedding function. The prior knowledge we use can be large amounts of data available in other tasks. This makes the FSL process more similar to human learning: the prior knowledge is like the common sense we use to make reasoning, and we learn task-specific information through limited data. <p>
        <p>Furthermore, I believe the power of FSL is what would make AI more democratic and available to everyone. Eventually, the privilege of training a large model will be controlled by large companies. But these AGI might not meet the customized need in industry. A common approach today is to fine tune a pretrained model, but even fine-tuning requires significant effort of tagging from day to night (some scenarios would not even have opportunity to create new data like the Lithium battery example I mentioned). FSL can help deploy the pretrained models and make AI accessible to everyone. Here, I made an example of using resnet152 as the embedding for prototypical network to predict Omniglot characters in a 50-way 5-shot learning task. The model did significantly better than naïve approach such as linear models or KNN even though the embedding has never been trained on the task (but prototypes are based on the Omniglot task).  <p>  
        <br>
        <a href="https://github.com/Xu-Zhang-Courant/Protonet_w_resnet152_embedding" > find out more here </a>
        <br>
        <br>
        <br>
        <br>
    </body>
    
<body>



<nav>
    <a href="#home">Home</a>
    <a href="#about">About</a>
    <a href="#services">Services</a>
    <a href="#contact">Contact</a>
</nav>

<footer>
    <p>&copy; Hello </p>
</footer>

</body>
</html>
